\documentclass[10pt,a4paper]{scrartcl}
\usepackage[utf8]{inputenc}
\usepackage[francais]{babel}
\usepackage{lmodern}
\usepackage[T1]{fontenc}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{amsmath, amssymb, amsthm}
\usepackage{geometry}
\usepackage{thmbox}
\usepackage{enumerate}
\usepackage{subcaption}

\geometry{
a4paper,
body={150mm,260mm},
left=30mm,top=15mm,
headheight=7mm,headsep=4mm,
marginparsep=4mm,
marginparwidth=27mm}


\pagestyle{empty}

\providecommand{\abs}[1]{\left|#1\right|}
\providecommand{\C}{\mathbb{C}}
\providecommand{\R}{\mathbb{R}}
\providecommand{\E}{\mathbb{E}}
\providecommand{\Prob}{\mathbb{P}}
\providecommand{\ii}{\mathrm{i}}
\providecommand{\w}{\omega}
\providecommand{\one}{\textbf{1}}

\renewcommand{\S}{\textbf{S}^n}

\newcommand{\norm}[1]{\Arrowvert#1\Arrowvert_2}

\newcount\colveccount

\newcommand*\colvec[1]{
        \global\colveccount#1
        \begin{pmatrix}
        \colvecnext
}
\def\colvecnext#1{
        #1
        \global\advance\colveccount-1
        \ifnum\colveccount>0
                \\
                \expandafter\colvecnext
        \else
                \end{pmatrix}
        \fi
}
 
\author{Judith Abecassis \& Timothée Lacroix}

\title{Random Graph Bandits with side information}


\begin{document}
\maketitle

\section{Introduction}
\subsection{Context}
We are interested in the multi-armed bandit with side information setting. This consists in a sequential learning where, at each iteration, the learner chooses an arm, and observes losses of a number of arms. This is a generalization of two well-studied feedback settings, one in which the learner only knows the loss of the arm it pulled (bandit setting), and the other where the losses of all arms are revealed (full information setting). Full information occurs for example for trading on a stock market: all stock prices are known, and the bandit setting can be illustrated by electronic advertising. A situation of partial revelation of other arms could be considered in advertising in a social network.

We are following the formalism previouly proposed by Mannor and Shamir (2011) in which side observations are modeled using a graph structure: each possible action is a node, and when an action is chosen, its loss and the losses of all its neighbours in the graph are revealed. Our work is in the line of Valko et al. (2015, under review), and considers the case where no previous information on the structure of the graph is known, and where partial observation is stochastic. 

We will be considering two models of random graphs.

\subsection{Different Graphs models}

\subsubsection{Erdos-Renyi}
Let $0\leq p \leq 1$. $E=ER(p)$ is the graph such that $(i \rightarrow j) \in E$ with probability $p$. Such graphs have very well understood properties. Their expected independence number $\bar{\alpha}$ is ????????.
Examples of ER graphs are given in Fig~\ref{er_ex}.

\begin{figure}[H]
\begin{center}
        \begin{subfigure}[b]{0.45\textwidth}
                \includegraphics[width=\textwidth]{figures/ER_graph_1.pdf}
                \caption{$p=0.1$}
        \end{subfigure}
	 \begin{subfigure}[b]{0.45\textwidth}
                \includegraphics[width=\textwidth]{figures/ER_graph_2.pdf}
                \caption{$p=0.2$}
        \end{subfigure}
	 \begin{subfigure}[b]{0.45\textwidth}
                \includegraphics[width=\textwidth]{figures/ER_graph_5.pdf}
                \caption{$p=0.5$}
        \end{subfigure}
	 \begin{subfigure}[b]{0.45\textwidth}
                \includegraphics[width=\textwidth]{figures/ER_graph_8.pdf}
                \caption{$p=0.8$}
        \end{subfigure}
\end{center}
\caption{Examples of Erdos-Renyi graphs for various $p$}
 \label{er_ex}
\end{figure}

In our learning setting, this corresponds to the fact that when an action is chosen, other actions independently reveal their losses with a constant probability $p$.

\subsubsection{Barabási-Albert}
A Barabási-Albert (BA) graph is constructed dynamically by linking a new node to pre-existing nodes, with a probability linear in the degree of these nodes. As such, a BA graph is parametric, with parameters $m$ and $m0$, namely the number of links a node creates when joining the network, and the size of the starting graph.

These graphs are scale-free, meaning that as they grow, their degree distribution follows a power law :
$$P(k) \equiv k^{-3}~\text{as}~N\rightarrow \infty$$

We used $m=m0=1$ to generate our graphs, leading to the exemples of graphs in figure~\ref{ba_ex}.

\begin{figure}[H]
\begin{center}
        \begin{subfigure}[b]{0.3\textwidth}
                \includegraphics[width=\textwidth]{figures/BA_graph_1.pdf}
                \caption{$p=0.1$}
        \end{subfigure}
	 \begin{subfigure}[b]{0.3\textwidth}
                \includegraphics[width=\textwidth]{figures/BA_graph_2.pdf}
                \caption{$p=0.2$}
        \end{subfigure}
	 \begin{subfigure}[b]{0.3\textwidth}
                \includegraphics[width=\textwidth]{figures/BA_graph_5.pdf}
                \caption{$p=0.2$}
        \end{subfigure}
\end{center}
\caption{Examples of Barabási-Albert graphs.}
 \label{ba_ex}
\end{figure}


In our learning setting, this models the phenomenon of preferential attachment in social networks.

\section{ER graphs}
\subsection{Idea of \textsc{DuplExp3} algorithm}
Valko et al. ahev developped an algorithm to learn in the bandit with side informations context without any prior knowledge on the graph, including the parameter of the graph $p$. The main trick used is to directly estimate the quantity $\frac{1}{p_{t,i}+(1-p_{t,i})r}$ through two independent geometric variable. This allows to get good loss estimate, and with an adequately tuned learnin rate, to achieve a regret of $O(\sqrt{(T/r)\log N})$ for high $p$.


In the general case, a separate 



\subsection{Examples of regret curves on ER graphs and comparison with EXP3}
First of all, we verify that our implementation of the algorithm DuplExp3 fares well against EXP3 on a reasonably sized graph. Fig~\ref{duplexp3vsexp3ER} shows the lower cumulated regret obtained by DuplExp3.
\begin{figure}[H]
\centering
\includegraphics[height=6cm]{figures/50new_dupl_big_r05.pdf}
\label{duplexp3vsexp3ER}
\caption{Comparison of cumulated regret over iterations for EXP3 and DuplExp3 algorithms for and ER graph of 50 nodes with parameter $p=0.5$ (repeated 100 times)}
\end{figure}

\section{BA graphs}
\subsection{Empirical properties of BA graphs}
\paragraph{Independence Number}
Considering the induced hub structure on BA graphs, we conjecture that for the same edge proportion, the mean independence number is higher for BA graphs than for ER graphs.  Our conjecture holds true empirically, as shown in Fig~\ref{mean_alpha_ba_er}.

\begin{figure}[H]
\centering
 \includegraphics[height=7cm]{figures/independance_number_com.pdf}
 \label{mean_alpha_ba_er}
 \caption{Average independence number for ER and BA graphs with equal edge proportion. We can see that the independence number of BA graphs stays above the independence number for ER graphs}
\end{figure}

\paragraph{Estimated r}
proof of $\E[M_t] \leq \frac{1}{p}$ ??
si pas proof et figure : figure
si rien ... rien


\subsection{Algorithms on BA graphs}
\subsubsection{Asymptotic regime}
As the number of nodes in a BA graph grows, the degree distribution becomes more and more independent of it's initial states and parameter. We can then compute the revelation probability for an arm :
$$r = \frac{\E[degree]}{N-1}$$
By setting the losses of EXP3 to be 
$$\hat{l}_{i,t} = \frac{O_{i,t}l_{i,t}}{p_{i,t}+(1-p_{i,t})r}$$
We then get non-biased loss estimates for the asymptotic regime.

%\begin{figure}
 %\includegraphics{figures/dupl_er_ba.eps}
 %\label{dupl_er_ba}
 %\caption{Average regret for DuplExp3 on BA and ER graphs with the same proportion of edges.}
%\end{figure}

\subsubsection{Finite regime}
\paragraph{DuplExp3 on BA Graphs}
To show that the conjecture on higher average independence numbers, and wrong revelation probability estimations indeed lead to an increased regret for \emph{DuplExp3}, we compare regrets of our implementations on ER and BA graphs with the same proportion of edges. Results are shown in Fig~\ref{dupl_er_ba}

\begin{figure}
 %\includegraphics{figures/dupl_er_ba.eps}
 \label{dupl_er_ba}
 \caption{Average regret for DuplExp3 on BA and ER graphs with the same proportion of edges.}
\end{figure}

We tried applying the asymptotic \emph{revelation probabillity} to small ($250$ nodes) BA graphs, which led to the plot in Fig~\ref{dupl_ba_finite_ba}.

\begin{figure}
% \includegraphics{figures/dupl_ba_finite_ba.eps} % computed on godzilla
 \label{dupl_ba_finite_ba}
 \caption{Regret for multiple algorithms on BA graphs of size $250$.}
\end{figure}


\section{Conclusion}
gargl


\end{document}